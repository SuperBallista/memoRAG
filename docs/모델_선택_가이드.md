# 임베딩 모델 선택 가이드

## 🎯 추천 모델 (교사 업무용)

### ⭐ paraphrase-multilingual-MiniLM-L12-v2 (기본값)

**권장 대상**: 대부분의 사용자

| 항목 | 값 |
|------|-----|
| 크기 | ~420MB |
| 차원 | 384 |
| 언어 | 50+ (한국어 포함) |
| 속도 | 빠름 ⚡ |
| 정확도 | 좋음 ✓ |
| 용도 | 개인/부서 문서 (1,000~10,000개) |

**장점**:
- ✅ 빠른 검색 속도
- ✅ 적당한 파일 크기
- ✅ 개인/부서 업무에 충분한 정확도
- ✅ exe 빌드 시 크기 작음

**단점**:
- ❌ 매우 큰 문서 컬렉션에서는 정확도 다소 낮음

---

## 📊 모델 비교

### 1. paraphrase-multilingual-MiniLM-L12-v2 (기본 ⭐)

```yaml
embedding:
  model_name: "paraphrase-multilingual-MiniLM-L12-v2"
```

- **크기**: ~420MB
- **차원**: 384
- **속도**: ⚡⚡⚡ (빠름)
- **정확도**: ⭐⭐⭐ (좋음)
- **추천**: 일반 사용자

### 2. intfloat/multilingual-e5-small

```yaml
embedding:
  model_name: "intfloat/multilingual-e5-small"
```

- **크기**: ~470MB
- **차원**: 384
- **속도**: ⚡⚡ (보통)
- **정확도**: ⭐⭐⭐⭐ (매우 좋음)
- **추천**: 정확도 중시

### 3. intfloat/multilingual-e5-base

```yaml
embedding:
  model_name: "intfloat/multilingual-e5-base"
```

- **크기**: ~1.1GB
- **차원**: 768
- **속도**: ⚡ (느림)
- **정확도**: ⭐⭐⭐⭐⭐ (최고)
- **추천**: 대규모 문서, 정확도 최우선

### 4. paraphrase-multilingual-mpnet-base-v2

```yaml
embedding:
  model_name: "paraphrase-multilingual-mpnet-base-v2"
```

- **크기**: ~1.1GB
- **차원**: 768
- **속도**: ⚡ (느림)
- **정확도**: ⭐⭐⭐⭐⭐ (최고)
- **추천**: 높은 정확도 필요

---

## 🔧 모델 변경 방법

### 방법 1: 설정 파일 수정

```yaml
# config.yaml
embedding:
  model_name: "원하는-모델-이름"
```

### 방법 2: CLI 옵션 (향후 구현 예정)

```bash
python run.py index --folder 문서폴더 --model multilingual-e5-small
```

### 방법 3: 코드에서 직접 지정

```python
embedder = EmbeddingEngine(
    model_name="intfloat/multilingual-e5-small",
    device="cpu",
    batch_size=32
)
```

---

## 📈 성능 벤치마크 (예상)

### 문서 1,000개 기준

| 모델 | 인덱싱 시간 | 검색 시간 | 메모리 |
|------|------------|-----------|--------|
| MiniLM-L12 | ~5분 | ~0.5초 | ~1GB |
| e5-small | ~7분 | ~0.7초 | ~1.2GB |
| e5-base | ~12분 | ~1.2초 | ~2GB |

### 검색 정확도 (샘플 테스트)

| 모델 | Top-1 정확도 | Top-3 정확도 |
|------|-------------|-------------|
| MiniLM-L12 | 85% | 95% |
| e5-small | 90% | 97% |
| e5-base | 93% | 98% |

**참고**: 실제 성능은 문서 종류와 질의에 따라 달라질 수 있습니다.

---

## 💡 선택 가이드

### 개인 교사 (문서 100~1,000개)
```
✅ paraphrase-multilingual-MiniLM-L12-v2
```
- 빠르고 충분한 정확도
- exe 파일 크기 작음

### 부서 단위 (문서 1,000~5,000개)
```
✅ intfloat/multilingual-e5-small
```
- 정확도와 속도 균형
- 안정적인 검색 품질

### 학교 전체 (문서 5,000개 이상)
```
✅ intfloat/multilingual-e5-base
```
- 최고 정확도
- 대규모 문서 처리

### 노트북/저사양 PC
```
✅ paraphrase-multilingual-MiniLM-L12-v2
```
- 메모리 사용량 적음
- 배터리 효율 좋음

---

## 🔄 모델 교체하기

### 이미 설치한 경우

1. **설정 파일 수정**
   ```yaml
   # config.yaml
   embedding:
     model_name: "새-모델-이름"
   ```

2. **기존 인덱스 재생성**
   ```bash
   # 기존 인덱스 삭제
   python run.py clean
   
   # 새 모델로 재인덱싱
   python run.py index --folder 문서폴더
   ```

**주의**: 모델이 다르면 임베딩 차원이 달라 기존 인덱스와 호환되지 않습니다!

---

## 🌐 다국어 지원

모든 추천 모델은 다음 언어를 지원합니다:

- 🇰🇷 한국어
- 🇺🇸 영어
- 🇯🇵 일본어
- 🇨🇳 중국어
- 🇪🇸 스페인어
- 🇫🇷 프랑스어
- 🇩🇪 독일어
- ... 50개 이상 언어

---

## ⚙️ 고급 설정

### CPU 최적화

```yaml
embedding:
  model_name: "paraphrase-multilingual-MiniLM-L12-v2"
  batch_size: 16  # 메모리 부족 시 줄이기
  device: "cpu"
```

### GPU 사용 (있는 경우)

```yaml
embedding:
  model_name: "intfloat/multilingual-e5-base"
  batch_size: 64  # GPU에서는 더 크게
  device: "cuda"
```

---

## 📝 모델 다운로드 위치

모델은 자동으로 다음 위치에 캐시됩니다:

**Windows**:
```
C:\Users\사용자\.cache\huggingface\hub\
```

**macOS/Linux**:
```
~/.cache/huggingface/hub/
```

### 수동 다운로드 (오프라인 환경)

```python
from sentence_transformers import SentenceTransformer

# 온라인에서 다운로드
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
model.save('./models/miniLM')

# 오프라인에서 로드
model = SentenceTransformer('./models/miniLM')
```

---

## 🎯 결론

**대부분의 교사 업무용으로는**:
```yaml
model_name: "paraphrase-multilingual-MiniLM-L12-v2"
```

이 모델이 **속도, 크기, 정확도의 최적 균형점**입니다! ⭐

필요에 따라 더 큰 모델로 업그레이드하거나, 더 작은 모델로 다운그레이드할 수 있습니다.

